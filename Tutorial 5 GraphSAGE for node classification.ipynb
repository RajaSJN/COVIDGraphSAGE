{"cells":[{"cell_type":"markdown","metadata":{"id":"13ee1898"},"source":["### Import DGL and PyTorch"],"id":"13ee1898"},{"cell_type":"markdown","metadata":{"id":"0bdf637a"},"source":["In this tutorial, we are going to introduce how to implement GraphSAGE to do semi-supervised learning on a node classification task. \n","\n","- We will first introduce how to implement and train a GraphSAGE model for node classification (`without the neighbor sampling step`).\n","- Then, we will show how to use DGL's sampler to enable the neighbor sampling to train and test the GraphSAGE model.\n","\n","We will demonstrate with the DGL package.\n","However, feel free to try on other packages such as PyTorch-Geometric.\n","\n","First, load pytorch, dgl, and other necessary packages (here we need to use NumPy)."],"id":"0bdf637a"},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2227,"status":"ok","timestamp":1666886676574,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"},"user_tz":-480},"id":"419a8d65"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"id":"419a8d65"},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8585,"status":"ok","timestamp":1666886685153,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"},"user_tz":-480},"id":"UVBn2NmgKhDD","outputId":"42b6f89a-0073-46d8-c152-e5b46efcf10c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.9.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (5.9.3)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.9.24)\n"]}],"source":["!pip install dgl"],"id":"UVBn2NmgKhDD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"07be9ca6"},"outputs":[],"source":["import dgl\n","from dgl import DGLGraph\n","\n","# Load Pytorch as backend\n","dgl.load_backend('pytorch')"],"id":"07be9ca6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"967441a3"},"outputs":[],"source":["import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')"],"id":"967441a3"},{"cell_type":"markdown","metadata":{"id":"a65e3312"},"source":["### Prepare the PubMed dataset\n","We use a citation network called pubmed for demonstration. A node in the citation network is a paper and an edge represents the citation between two papers. \n","\n","This dataset has 19,717 papers and 88,651 citations. Each paper has a sparse bag-of-words feature vector and a class label."],"id":"a65e3312"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cc17ffb"},"outputs":[],"source":["from dgl.data import citegrh\n","\n","# load and preprocess the pubmed dataset\n","data = citegrh.load_pubmed()\n","\n","# sparse bag-of-words features of papers\n","features = torch.FloatTensor(data.features)\n","# the number of input node features\n","in_feats = features.shape[1]\n","# class labels of papers\n","labels = torch.LongTensor(data.labels)\n","# the number of unique classes on the nodes.\n","n_classes = data.num_labels"],"id":"4cc17ffb"},{"cell_type":"markdown","metadata":{"id":"3edf5de7"},"source":["Here we remove all self-loops in the graph."],"id":"3edf5de7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"39a91a7c"},"outputs":[],"source":["data[0]"],"id":"39a91a7c"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5c301336"},"outputs":[],"source":["graph = dgl.remove_self_loop(data[0])\n","graph"],"id":"5c301336"},{"cell_type":"markdown","metadata":{"id":"46bf6755"},"source":["### Implement the GNN model\n","\n","Essentially, given a graph structure, GNNs (GCN, GraphSAGE, GAT, etc.) are used to learn meaningful node representations (in this case, the embeddings, or vectors).\n","Once these embeddings are properly learnt, we may perform downstream tasks such as node classification, graph classification, and link prediction.\n","\n","DGL provides two ways of implementing a GNN model:\n","\n","- using the nn module, which contains many commonly used GNN modules.\n","- using the message passing interface to implement a GNN model from scratch.\n","\n","For simplicity, we implement the GraphSAGE model in the tutorial with the nn module.\n","\n","If you are interested in using the message passing interface to implement a GNN model, check this link https://doc.dgl.ai/tutorials/models/index.html out.\n","\n","![fishy](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/GNN.png)\n","\n","The GraphSage model has multiple layers. In each layer, a vertex accesses its direct neighbors. When we stack $k$ layers in a model, a node $v$ access neighbors within $k$ hops. The output of the GraphSage model is **node embeddings** that represent the nodes and all information in the k-hop neighborhood.\n","\n","If you want to learn about the details of the SageConv layer, look at its official documantation at https://docs.dgl.ai/en/0.8.x/generated/dgl.nn.pytorch.conv.SAGEConv.html "],"id":"46bf6755"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3565e369"},"outputs":[],"source":["from dgl.nn.pytorch import conv as dgl_conv\n","\n","class GraphSAGEModel(nn.Module):\n","    def __init__(self,\n","                 in_feats,\n","                 n_hidden,\n","                 out_dim,\n","                 n_layers,\n","                 activation,\n","                 dropout,\n","                 aggregator_type):\n","        super(GraphSAGEModel, self).__init__()\n","        self.layers = nn.ModuleList()\n","\n","        # input layer\n","        self.layers.append(dgl_conv.SAGEConv(in_feats, n_hidden, aggregator_type,\n","                                         feat_drop=dropout, activation=activation))\n","        # hidden layers\n","        for i in range(n_layers - 1):\n","            self.layers.append(dgl_conv.SAGEConv(n_hidden, n_hidden, aggregator_type,\n","                                             feat_drop=dropout, activation=activation))\n","        # output layer\n","        self.layers.append(dgl_conv.SAGEConv(n_hidden, out_dim, aggregator_type,\n","                                         feat_drop=dropout, activation=None))\n","\n","    def forward(self, g, features):\n","        h = features\n","        for layer in self.layers:\n","            h = layer(g, h)\n","        return h"],"id":"3565e369"},{"cell_type":"markdown","metadata":{"id":"b504e26e"},"source":["### Node classification (semi-supervised)\n","Let us perform node classification in a semi-supervised setting. In this setting, we have the entire graph structure and all node features. We only have labels on some of the nodes. We want to predict the labels on other nodes. Even though some of the nodes do not have labels, they connect with nodes with labels. Thus, we train the model with both labeled nodes and unlabeled nodes. Semi-supervised learning can usually improve performance.\n","\n","![semisupervised](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/node_classify1.png)\n","\n","This dependency graph shows a better view of how labeled and unlabled nodes are used in the training. \n","\n","![dependency](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/node_classify2.png)"],"id":"b504e26e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6c81b7b"},"outputs":[],"source":["# Hyperparameters\n","n_hidden = 64\n","n_layers = 2\n","dropout = 0.5\n","aggregator_type = 'mean'\n","\n","gconv_model = GraphSAGEModel(in_feats,\n","                             n_hidden,\n","                             n_classes,\n","                             n_layers,\n","                             F.relu,\n","                             dropout,\n","                             aggregator_type)"],"id":"c6c81b7b"},{"cell_type":"markdown","metadata":{"id":"0800fd28"},"source":["Now we create the node classification model based on the GraphSage model. The GraphSage model takes a DGLGraph object and node features as input and computes node embeddings as output. With node embeddings, we use a cross entropy loss to train the node classification model."],"id":"0800fd28"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c37985dd"},"outputs":[],"source":["class NodeClassification(nn.Module):\n","    def __init__(self, gconv_model, n_hidden, n_classes):\n","        super(NodeClassification, self).__init__()\n","        self.gconv_model = gconv_model\n","        self.loss_fcn = torch.nn.CrossEntropyLoss()\n","\n","    def forward(self, g, features, train_mask):\n","        logits = self.gconv_model(g, features)\n","        return self.loss_fcn(logits[train_mask], labels[train_mask])"],"id":"c37985dd"},{"cell_type":"markdown","metadata":{"id":"bffeda9e"},"source":["After defining a model for node classification, we define the evaluation, train and test function."],"id":"bffeda9e"},{"cell_type":"code","execution_count":null,"metadata":{"id":"89fd66d4"},"outputs":[],"source":["def NCEvaluate(model, g, features, labels, test_mask):\n","    model.eval()\n","    with torch.no_grad():\n","        # compute embeddings with GNN\n","        logits = model.gconv_model(g, features)\n","        logits = logits[test_mask]\n","        test_labels = labels[test_mask]\n","        _, indices = torch.max(logits, dim=1)\n","        correct = torch.sum(indices == test_labels)\n","        acc = correct.item() * 1.0 / len(test_labels)\n","    return acc\n","\n","def Train(model, graph, features, train_mask, val_mask, labels, n_epochs):\n","    for epoch in range(n_epochs):\n","        # Set the model in the training mode.\n","        model.train()\n","        # forward\n","        loss = model(graph, features, train_mask)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        acc = NCEvaluate(model, graph, features, labels, val_mask)\n","        print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f}\"\n","            .format(epoch, loss.item(), acc))\n","\n","def Test(model, graph, features, labels, test_mask):\n","    print('Testing Accuracy:', NCEvaluate(model, graph, features, labels, test_mask))"],"id":"89fd66d4"},{"cell_type":"markdown","metadata":{"id":"8f892a9f"},"source":["Prepare data for semi-supervised node classification"],"id":"8f892a9f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"10a803e2"},"outputs":[],"source":["train_mask = graph.ndata['train_mask']\n","val_mask = graph.ndata['val_mask']\n","test_mask = graph.ndata['test_mask']\n","\n","print(\"\"\"----Data statistics------'\n","      #Classes {}\n","      #Train samples {}\n","      #Val samples {}\n","      #Test samples {}\"\"\".format(\n","          n_classes,\n","           data.train_mask.sum().item(),\n","           data.val_mask.sum().item(),\n","           data.test_mask.sum().item()))"],"id":"10a803e2"},{"cell_type":"markdown","metadata":{"id":"d9395759"},"source":["After defining the model and evaluation function, we can put everything into the training loop to train the model."],"id":"d9395759"},{"cell_type":"code","execution_count":null,"metadata":{"id":"36b9ebbc"},"outputs":[],"source":["# Node classification task\n","model = NodeClassification(gconv_model, n_hidden, n_classes)\n","\n","# Training hyperparameters\n","weight_decay = 5e-4\n","n_epochs = 150\n","lr = 1e-3\n","\n","# create the Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","Train(model, graph, features, train_mask, val_mask, labels, n_epochs)\n","Test(model, graph, features, labels, test_mask)\n"],"id":"36b9ebbc"},{"cell_type":"markdown","metadata":{"id":"cc102d2f"},"source":["### Train with neighbor sampling\n","The above example runs without neighbor sampling.\n","Now, let's look at how to implement this feature.\n","\n","DGL has implemented this for us in functions:\n","\n","`dgl.dataloading.MultiLayerNeighborSampler`\n","and\n","`dgl.dataloading.NodeDataLoader`\n","\n","Note that **the GraphSAGE structure does not change**, it is only a change in the training fashion: \n","\n","(1) we change to batched training, and \n","\n","(2) each node within a batch is updated with a portion of randomly sampled neighbors instead of all its neighbors."],"id":"cc102d2f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"87d6be25"},"outputs":[],"source":["batch_size = 1024\n","fan_out = [10, 20, 30]  # maximum number of neighbors in each \n","\n","sampler = dgl.dataloading.MultiLayerNeighborSampler(fan_out)\n","\n","def get_dataloader_with_sampling(graph, mask, sampler, batch_size=32, shuffle=False):\n","    nids = torch.where(mask==True)[0]\n","    dataloader = dgl.dataloading.NodeDataLoader(\n","                    graph,\n","                    nids,\n","                    sampler,\n","                    batch_size=batch_size,\n","                    shuffle=shuffle,\n","                    drop_last=False,\n","                    num_workers=8)\n","    return dataloader\n","\n","\n","train_dataloader = get_dataloader_with_sampling(graph, train_mask, sampler, batch_size, True)\n","val_dataloader = get_dataloader_with_sampling(graph, val_mask, sampler, batch_size, False)\n","test_dataloader = get_dataloader_with_sampling(graph, test_mask, sampler, batch_size, False)"],"id":"87d6be25"},{"cell_type":"markdown","metadata":{"id":"368425f3"},"source":["The model structure remains the same. \n","And the only difference is at the **forward** function, where we adapt the function to receive `blocks` data as inputs, which are the batched neighborhood-sampled graphs."],"id":"368425f3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7c3521e"},"outputs":[],"source":["class GraphSAGEModel(nn.Module):\n","    def __init__(self,\n","                 in_feats,\n","                 n_hidden,\n","                 out_dim,\n","                 n_layers,\n","                 activation,\n","                 dropout,\n","                 aggregator_type):\n","        super(GraphSAGEModel, self).__init__()\n","        self.layers = nn.ModuleList()\n","\n","        # input layer\n","        self.layers.append(dgl_conv.SAGEConv(in_feats, n_hidden, aggregator_type,\n","                                         feat_drop=dropout, activation=activation))\n","        # hidden layers\n","        for i in range(n_layers - 1):\n","            self.layers.append(dgl_conv.SAGEConv(n_hidden, n_hidden, aggregator_type,\n","                                             feat_drop=dropout, activation=activation))\n","        # output layer\n","        self.layers.append(dgl_conv.SAGEConv(n_hidden, out_dim, aggregator_type,\n","                                         feat_drop=dropout, activation=None))\n","\n","    ''' Notice the difference in the forward method'''\n","    def forward(self, blocks, features):\n","        h = features\n","        for layer, block in zip(self.layers, blocks):\n","            h = layer(block, h)\n","        return h\n","\n","    ''' This is the forward method for no-sampling '''\n","    # def forward(self, g, features):\n","    #     h = features\n","    #     for layer in self.layers:\n","    #         h = layer(g, h)\n","    #     return h\n","\n","\n","class NodeClassification(nn.Module):\n","    def __init__(self, gconv_model):\n","        super(NodeClassification, self).__init__()\n","        self.gconv_model = gconv_model\n","        self.loss_fcn = torch.nn.CrossEntropyLoss()\n","\n","    ''' Instead of using masks, we pass the features and labels corresponding to the blocks for batched learning. '''\n","    def forward(self, blocks, features, labels):\n","        logits = self.gconv_model(blocks, features)\n","        return self.loss_fcn(logits, labels)\n","\n"],"id":"f7c3521e"},{"cell_type":"markdown","metadata":{"id":"f9e9c6fe"},"source":["For the training and evaluation, we re-organize them to receive batch input."],"id":"f9e9c6fe"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7e73ae9"},"outputs":[],"source":["def Evaluate(model, eval_dataloader):\n","    model.eval()\n","    with torch.no_grad():\n","        all_labels = []\n","        all_logits = []\n","        for batch in eval_dataloader:\n","            input_nodes, output_nodes, blocks = batch\n","            x = blocks[0].srcdata[\"feat\"]\n","            y = blocks[-1].dstdata[\"label\"]\n","            batch_logits = model.gconv_model(blocks, x)\n","\n","            all_logits.append(batch_logits)\n","            all_labels.append(y)\n","\n","        labels = torch.cat(all_labels)\n","        logits = torch.cat(all_logits)\n","        \n","        # compute metrics: Accuracy\n","        _, indices = torch.max(logits, dim=1)\n","        correct = torch.sum(indices == labels)\n","        acc = correct.item() * 1.0 / len(labels)\n","    return acc\n","\n","def Train(model, train_dataloader, val_dataloader, optimizer, n_epochs):\n","    for epoch in range(n_epochs):\n","        # Set the model in the training mode.\n","        model.train()\n","    \n","        # # forward (no sampling)\n","        # loss = model(graph, features, train_mask)\n","        # optimizer.zero_grad()\n","        # loss.backward()\n","        # optimizer.step()\n","\n","        # forward\n","        for batch in train_dataloader:\n","            input_nodes, output_nodes, blocks = batch\n","            x = blocks[0].srcdata[\"feat\"]\n","            y = blocks[-1].dstdata[\"label\"]\n","            loss = model(blocks, x, y)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        # evaluate on the validation set\n","        acc = Evaluate(model, val_dataloader)\n","        print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f}\"\n","            .format(epoch, loss.item(), acc))\n","\n","def Test(model, test_dataloader):\n","    acc = Evaluate(model, test_dataloader)\n","    print('Testing Accuracy', acc)"],"id":"c7e73ae9"},{"cell_type":"markdown","metadata":{"id":"052391a2"},"source":["Let's try training a model in this way..."],"id":"052391a2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ca1ceae6"},"outputs":[],"source":["# Hyperparameters\n","n_hidden = 64\n","n_layers = 2\n","dropout = 0.5\n","aggregator_type = 'mean'\n","\n","gconv_model = GraphSAGEModel(in_feats,\n","                             n_hidden,\n","                             n_classes,\n","                             n_layers,\n","                             F.relu,\n","                             dropout,\n","                             aggregator_type)\n","\n","# Node classification task\n","model = NodeClassification(gconv_model)"],"id":"ca1ceae6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"be92fede"},"outputs":[],"source":["# Training hyperparameters\n","weight_decay = 5e-4\n","n_epochs = 150\n","lr = 1e-3\n","\n","# create the Adam optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","Train(model, train_dataloader, val_dataloader, optimizer, n_epochs)\n","Test(model, test_dataloader)"],"id":"be92fede"},{"cell_type":"markdown","metadata":{"id":"624d656d"},"source":["Generally, the results should be very similar to the previous one on this dataset."],"id":"624d656d"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":5}