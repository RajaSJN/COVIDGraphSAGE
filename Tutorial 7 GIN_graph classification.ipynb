{"cells":[{"cell_type":"markdown","metadata":{"id":"wD4yseCeb_fV"},"source":["### Import DGL and PyTorch"],"id":"wD4yseCeb_fV"},{"cell_type":"markdown","metadata":{"id":"Oq1rLDARb_fY"},"source":["In this tutorial, we are going to introduce how to implement GAT for link prediction task. \n"],"id":"Oq1rLDARb_fY"},{"cell_type":"code","execution_count":2,"metadata":{"id":"ayciP7QZb_fZ","executionInfo":{"status":"ok","timestamp":1666875970016,"user_tz":-480,"elapsed":2,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"id":"ayciP7QZb_fZ"},{"cell_type":"code","source":["!pip install dgl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEt7fmw_dlR9","executionInfo":{"status":"ok","timestamp":1666875976110,"user_tz":-480,"elapsed":5552,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"47414c85-a1a3-4087-b49f-3083c2132682"},"id":"iEt7fmw_dlR9","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: dgl in /usr/local/lib/python3.7/dist-packages (0.9.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.21.6)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (5.9.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.23.0)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl) (2.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl) (4.64.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl) (1.7.3)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qReFtTndb_fa","executionInfo":{"status":"ok","timestamp":1666875976628,"user_tz":-480,"elapsed":537,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"9c883094-4df4-4085-fea5-a74bf40ab17a"},"outputs":[{"output_type":"stream","name":"stderr","text":["DGL backend not selected or invalid.  Assuming PyTorch for now.\n"]},{"output_type":"stream","name":"stdout","text":["Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"]}],"source":["import dgl\n","from dgl import DGLGraph"],"id":"qReFtTndb_fa"},{"cell_type":"code","execution_count":5,"metadata":{"id":"i7rfG7YAb_fa","executionInfo":{"status":"ok","timestamp":1666875976629,"user_tz":-480,"elapsed":3,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')"],"id":"i7rfG7YAb_fa"},{"cell_type":"markdown","metadata":{"id":"ZIG3EAIKb_fb"},"source":["### Prepare the MUTAG dataset\n","We use a collection of nitroaromatic compounds called MUTAG for demonstration and the goal is to predict their mutagenicity on Salmonella typhimurium. Input graphs are used to represent chemical compounds, where vertices stand for atoms and are labeled by the atom type, while edges between vertices represent bonds between the corresponding atoms.\n","It includes 188 samples of chemical compounds with 7 discrete node labels."],"id":"ZIG3EAIKb_fb"},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCA5PGPrb_fb","executionInfo":{"status":"ok","timestamp":1666875980355,"user_tz":-480,"elapsed":3729,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"5cf1a557-77d7-4988-f452-746eb73ae623"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading /root/.dgl/GINDataset.zip from https://raw.githubusercontent.com/weihua916/powerful-gnns/master/dataset.zip...\n","Extracting file to /root/.dgl/GINDataset\n"]}],"source":["from dgl.data import GINDataset\n","\n","device = 'cpu'\n","# load and preprocess the pubmed dataset\n","dataset = GINDataset('MUTAG', self_loop=True, degree_as_nlabel=False)\n","# the number of input node features\n","in_feats = dataset.dim_nfeats\n","# class labels of papers\n","labels = [l for _, l in dataset]"],"id":"rCA5PGPrb_fb"},{"cell_type":"markdown","metadata":{"id":"sO9IEHWQb_fc"},"source":["Here we add self-loops in the graph for self-attention"],"id":"sO9IEHWQb_fc"},{"cell_type":"code","source":["print(dataset[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1BeGskqkYCU","executionInfo":{"status":"ok","timestamp":1666877312359,"user_tz":-480,"elapsed":370,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"89202ae1-7182-4d25-9cea-fadf8643b32a"},"id":"v1BeGskqkYCU","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["(Graph(num_nodes=23, num_edges=77,\n","      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(7,), dtype=torch.float32)}\n","      edata_schemes={}), tensor(0))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"TAusMwk9kcvc"},"id":"TAusMwk9kcvc","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8K3F-DG-b_fc","executionInfo":{"status":"ok","timestamp":1666875980835,"user_tz":-480,"elapsed":483,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"db393f91-09d7-4891-a922-0d2e134b3fc1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 1., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{},"execution_count":7}],"source":["dataset[0][0].ndata['attr']"],"id":"8K3F-DG-b_fc"},{"cell_type":"markdown","metadata":{"id":"U1pZkky5b_fd"},"source":["we split the dataset for train and test."],"id":"U1pZkky5b_fd"},{"cell_type":"code","execution_count":8,"metadata":{"id":"krgD8k15b_fd","executionInfo":{"status":"ok","timestamp":1666875980836,"user_tz":-480,"elapsed":5,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train_idx, val_idx = train_test_split(np.arange(len(dataset)),test_size=0.1)"],"id":"krgD8k15b_fd"},{"cell_type":"code","source":["print(train_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lf28pyGpkT_8","executionInfo":{"status":"ok","timestamp":1666876268185,"user_tz":-480,"elapsed":306,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"0162acb8-5e78-4ce0-c971-f3cc0631882c"},"id":"lf28pyGpkT_8","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[150  46  85  43  81 129  54 139  59  29  67 116  86  35  38 173   7  89\n","  92 115  95 147  32 100  15 135 154  56  24 143 113 106  58  78  50  70\n"," 145  41  55  96 122 118 141 151 127 133  25  17 109  76  72 112 119 144\n","  44 148 161 103 176  62 134  36 104 175  57  99 114 136 166  87   5  28\n"," 146 169 153 155 185  61  12  53  65 171  26  68  47  23  75 170  60 123\n"," 172   3  19  31  11 159  93 131  79 121  52  69 108   9 137 174 130  42\n"," 107 157  22 124   8 180  27  16  84  20  63  37   4 111 158  98 105  51\n"," 126 187  48  10 182 156  33 140 152 167 128  21  91  49  83 165 110  45\n"," 101 132   0  18 177 164  64 162 181  40 179  34   2 102  13  66   6  97\n"," 163  39 120 117 178 183  80]\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"id":"C9du0wi1b_fe","executionInfo":{"status":"ok","timestamp":1666875980836,"user_tz":-480,"elapsed":3,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","from dgl.dataloading import GraphDataLoader\n","\n","train_loader = GraphDataLoader(\n","    dataset,\n","    sampler=SubsetRandomSampler(train_idx),\n","    batch_size=128,\n",")\n","val_loader = GraphDataLoader(\n","    dataset,\n","    sampler=SubsetRandomSampler(val_idx),\n","    batch_size=128,\n",")"],"id":"C9du0wi1b_fe"},{"cell_type":"markdown","metadata":{"id":"gFR5KKhGb_fe"},"source":["### Implement the GNN model\n","\n","Essentially, given a graph structure, GNNs (GCN, GraphSAGE, GAT, etc.) are used to learn meaningful node representations (in this case, the embeddings, or vectors).\n","Once these embeddings are properly learnt, we may perform downstream tasks such as node classification, graph classification, and link prediction.\n","\n","DGL provides two ways of implementing a GNN model:\n","\n","- using the nn module, which contains many commonly used GNN modules.\n","- using the message passing interface to implement a GNN model from scratch.\n","\n","If you are interested in using the message passing interface to implement a GNN model, check this link https://doc.dgl.ai/tutorials/models/index.html out.\n","\n","![fishy](https://raw.githubusercontent.com/dglai/WWW20-Hands-on-Tutorial/master/images/GNN.png)"],"id":"gFR5KKhGb_fe"},{"cell_type":"markdown","metadata":{"id":"WbbHQnskb_ff"},"source":["Define GIN model."],"id":"WbbHQnskb_ff"},{"cell_type":"code","execution_count":10,"metadata":{"id":"UyvC-rDrb_ff","executionInfo":{"status":"ok","timestamp":1666875980836,"user_tz":-480,"elapsed":2,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["class MLP(nn.Module):\n","    \"\"\"Construct two-layer MLP-type aggreator for GIN model\"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.linears = nn.ModuleList()\n","        # two-layer MLP\n","        self.linears.append(nn.Linear(input_dim, hidden_dim, bias=False))\n","        self.linears.append(nn.Linear(hidden_dim, output_dim, bias=False))\n","        self.batch_norm = nn.BatchNorm1d((hidden_dim))\n","\n","    def forward(self, x):\n","        h = x\n","        h = F.relu(self.batch_norm(self.linears[0](h)))\n","        return self.linears[1](h)\n","\n","\n","from dgl.nn.pytorch.conv import GINConv\n","from dgl.nn.pytorch.glob import SumPooling\n","    \n","class GIN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.ginlayers = nn.ModuleList()\n","        self.batch_norms = nn.ModuleList()\n","        num_layers = 5\n","        # five-layer GCN with two-layer MLP aggregator and sum-neighbor-pooling scheme\n","        for layer in range(num_layers - 1):  # excluding the input layer\n","            if layer == 0:\n","                mlp = MLP(input_dim, hidden_dim, hidden_dim)\n","            else:\n","                mlp = MLP(hidden_dim, hidden_dim, hidden_dim)\n","            self.ginlayers.append(\n","                GINConv(mlp, learn_eps=False)\n","            )  # set to True if learning epsilon\n","            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n","        # linear functions for graph sum poolings of output of each layer\n","        self.linear_prediction = nn.ModuleList()\n","        for layer in range(num_layers):\n","            if layer == 0:\n","                self.linear_prediction.append(nn.Linear(input_dim, output_dim))\n","            else:\n","                self.linear_prediction.append(nn.Linear(hidden_dim, output_dim))\n","        self.drop = nn.Dropout(0.5)\n","        self.pool = (\n","            SumPooling()\n","        )  # change to mean readout (AvgPooling) on social network datasets\n","\n","    def forward(self, g, h):\n","        # list of hidden representation at each layer (including the input layer)\n","        hidden_rep = [h]\n","        for i, layer in enumerate(self.ginlayers):\n","            h = layer(g, h)\n","            h = self.batch_norms[i](h)\n","            h = F.relu(h)\n","            hidden_rep.append(h)\n","        score_over_layer = 0\n","        # perform graph sum pooling over all nodes in each layer\n","        for i, h in enumerate(hidden_rep):\n","            pooled_h = self.pool(g, h)\n","            score_over_layer += self.drop(self.linear_prediction[i](pooled_h))\n","        return score_over_layer"],"id":"UyvC-rDrb_ff"},{"cell_type":"markdown","metadata":{"id":"zmcwD5sGb_fg"},"source":["### Graph Classification\n","\n","![semisupervised](https://data.dgl.ai/tutorial/batch/graph_classifier.png)\n"],"id":"zmcwD5sGb_fg"},{"cell_type":"code","execution_count":11,"metadata":{"id":"dSj8l0J7b_fh","executionInfo":{"status":"ok","timestamp":1666875980836,"user_tz":-480,"elapsed":2,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["# Hyperparameters\n","in_size = dataset.dim_nfeats\n","out_size = dataset.gclasses\n","hidden_size = 16\n","model = GIN(in_size, hidden_size, out_size).to(device)\n"],"id":"dSj8l0J7b_fh"},{"cell_type":"markdown","metadata":{"id":"Aqyh5Qyob_fh"},"source":["After defining a model for graph classification, we define evaluation function."],"id":"Aqyh5Qyob_fh"},{"cell_type":"code","execution_count":12,"metadata":{"id":"-dyHhlVUb_fi","executionInfo":{"status":"ok","timestamp":1666875981154,"user_tz":-480,"elapsed":320,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}}},"outputs":[],"source":["def evaluate(dataloader, device, model):\n","    model.eval()\n","    total = 0\n","    total_correct = 0\n","    for batched_graph, labels in dataloader:\n","        batched_graph = batched_graph.to(device)\n","        labels = labels.to(device)\n","        feat = batched_graph.ndata.pop(\"attr\")\n","        total += len(labels)\n","        logits = model(batched_graph, feat)\n","        _, predicted = torch.max(logits, 1)\n","        total_correct += (predicted == labels).sum().item()\n","    acc = 1.0 * total_correct / total\n","    return acc"],"id":"-dyHhlVUb_fi"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aXo23TIb_fi","executionInfo":{"status":"ok","timestamp":1666876010091,"user_tz":-480,"elapsed":21511,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"89285dde-2fd6-4af8-85eb-ff7f252b965a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 00000 | Loss 5.2796 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00001 | Loss 2.4202 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00002 | Loss 1.7866 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00003 | Loss 1.5285 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00004 | Loss 1.4200 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00005 | Loss 1.3061 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00006 | Loss 1.2221 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00007 | Loss 0.9368 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00008 | Loss 1.0632 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00009 | Loss 1.5010 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00010 | Loss 0.7096 | Train Acc. 0.6746 | Validation Acc. 0.5789 \n","Epoch 00011 | Loss 1.0330 | Train Acc. 0.6805 | Validation Acc. 0.5789 \n","Epoch 00012 | Loss 0.7177 | Train Acc. 0.6805 | Validation Acc. 0.6316 \n","Epoch 00013 | Loss 0.5728 | Train Acc. 0.6864 | Validation Acc. 0.6316 \n","Epoch 00014 | Loss 0.6367 | Train Acc. 0.6864 | Validation Acc. 0.6316 \n","Epoch 00015 | Loss 0.4059 | Train Acc. 0.6923 | Validation Acc. 0.6316 \n","Epoch 00016 | Loss 0.7767 | Train Acc. 0.7041 | Validation Acc. 0.6316 \n","Epoch 00017 | Loss 0.6519 | Train Acc. 0.7101 | Validation Acc. 0.6316 \n","Epoch 00018 | Loss 0.6798 | Train Acc. 0.7160 | Validation Acc. 0.6316 \n","Epoch 00019 | Loss 0.5657 | Train Acc. 0.7456 | Validation Acc. 0.8947 \n","Epoch 00020 | Loss 0.6132 | Train Acc. 0.7929 | Validation Acc. 0.8947 \n","Epoch 00021 | Loss 0.4942 | Train Acc. 0.8107 | Validation Acc. 0.9474 \n","Epoch 00022 | Loss 0.4449 | Train Acc. 0.8580 | Validation Acc. 0.9474 \n","Epoch 00023 | Loss 0.4590 | Train Acc. 0.8876 | Validation Acc. 0.9474 \n","Epoch 00024 | Loss 0.4391 | Train Acc. 0.8876 | Validation Acc. 0.8947 \n","Epoch 00025 | Loss 0.6743 | Train Acc. 0.8580 | Validation Acc. 0.8947 \n","Epoch 00026 | Loss 0.4996 | Train Acc. 0.9053 | Validation Acc. 0.9474 \n","Epoch 00027 | Loss 0.5891 | Train Acc. 0.8935 | Validation Acc. 0.8947 \n","Epoch 00028 | Loss 0.3837 | Train Acc. 0.8107 | Validation Acc. 0.8947 \n","Epoch 00029 | Loss 0.6404 | Train Acc. 0.8639 | Validation Acc. 0.8947 \n","Epoch 00030 | Loss 0.3138 | Train Acc. 0.9053 | Validation Acc. 0.9474 \n","Epoch 00031 | Loss 0.4491 | Train Acc. 0.8935 | Validation Acc. 0.9474 \n","Epoch 00032 | Loss 0.4385 | Train Acc. 0.8935 | Validation Acc. 0.9474 \n","Epoch 00033 | Loss 0.3366 | Train Acc. 0.8580 | Validation Acc. 0.7368 \n","Epoch 00034 | Loss 0.4907 | Train Acc. 0.9112 | Validation Acc. 0.9474 \n","Epoch 00035 | Loss 0.3454 | Train Acc. 0.8166 | Validation Acc. 0.8421 \n","Epoch 00036 | Loss 0.2652 | Train Acc. 0.7988 | Validation Acc. 0.7368 \n","Epoch 00037 | Loss 0.4291 | Train Acc. 0.8698 | Validation Acc. 0.9474 \n","Epoch 00038 | Loss 0.3839 | Train Acc. 0.9112 | Validation Acc. 0.9474 \n","Epoch 00039 | Loss 0.2702 | Train Acc. 0.9112 | Validation Acc. 0.9474 \n","Epoch 00040 | Loss 0.2652 | Train Acc. 0.9172 | Validation Acc. 0.9474 \n","Epoch 00041 | Loss 0.3072 | Train Acc. 0.9172 | Validation Acc. 0.9474 \n","Epoch 00042 | Loss 0.3001 | Train Acc. 0.9112 | Validation Acc. 0.9474 \n","Epoch 00043 | Loss 0.1752 | Train Acc. 0.9053 | Validation Acc. 0.9474 \n","Epoch 00044 | Loss 0.3410 | Train Acc. 0.8994 | Validation Acc. 0.9474 \n","Epoch 00045 | Loss 0.2550 | Train Acc. 0.8817 | Validation Acc. 0.9474 \n","Epoch 00046 | Loss 0.2656 | Train Acc. 0.9527 | Validation Acc. 0.9474 \n","Epoch 00047 | Loss 0.2635 | Train Acc. 0.9349 | Validation Acc. 0.8947 \n","Epoch 00048 | Loss 0.1922 | Train Acc. 0.9112 | Validation Acc. 0.7895 \n","Epoch 00049 | Loss 0.2517 | Train Acc. 0.8343 | Validation Acc. 0.8421 \n","Epoch 00050 | Loss 0.2139 | Train Acc. 0.8343 | Validation Acc. 0.8421 \n","Epoch 00051 | Loss 0.3062 | Train Acc. 0.8580 | Validation Acc. 0.8421 \n","Epoch 00052 | Loss 0.1987 | Train Acc. 0.8402 | Validation Acc. 0.7895 \n","Epoch 00053 | Loss 0.2184 | Train Acc. 0.8402 | Validation Acc. 0.8421 \n","Epoch 00054 | Loss 0.3068 | Train Acc. 0.8462 | Validation Acc. 0.8421 \n","Epoch 00055 | Loss 0.1645 | Train Acc. 0.9172 | Validation Acc. 0.8947 \n","Epoch 00056 | Loss 0.1921 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00057 | Loss 0.1388 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00058 | Loss 0.2090 | Train Acc. 0.9290 | Validation Acc. 0.8947 \n","Epoch 00059 | Loss 0.3554 | Train Acc. 0.9349 | Validation Acc. 0.8947 \n","Epoch 00060 | Loss 0.2036 | Train Acc. 0.9467 | Validation Acc. 0.8947 \n","Epoch 00061 | Loss 0.1686 | Train Acc. 0.9527 | Validation Acc. 0.8947 \n","Epoch 00062 | Loss 0.2129 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00063 | Loss 0.1958 | Train Acc. 0.9231 | Validation Acc. 0.9474 \n","Epoch 00064 | Loss 0.2816 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00065 | Loss 0.1450 | Train Acc. 0.9349 | Validation Acc. 0.9474 \n","Epoch 00066 | Loss 0.1696 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00067 | Loss 0.2040 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00068 | Loss 0.1635 | Train Acc. 0.9349 | Validation Acc. 0.8947 \n","Epoch 00069 | Loss 0.1772 | Train Acc. 0.9112 | Validation Acc. 0.8947 \n","Epoch 00070 | Loss 0.1887 | Train Acc. 0.9172 | Validation Acc. 0.8947 \n","Epoch 00071 | Loss 0.1411 | Train Acc. 0.9290 | Validation Acc. 0.8947 \n","Epoch 00072 | Loss 0.2980 | Train Acc. 0.9467 | Validation Acc. 0.8947 \n","Epoch 00073 | Loss 0.1983 | Train Acc. 0.9467 | Validation Acc. 0.8947 \n","Epoch 00074 | Loss 0.2142 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00075 | Loss 0.1649 | Train Acc. 0.9527 | Validation Acc. 0.9474 \n","Epoch 00076 | Loss 0.1140 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00077 | Loss 0.1293 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00078 | Loss 0.1505 | Train Acc. 0.9349 | Validation Acc. 0.9474 \n","Epoch 00079 | Loss 0.2035 | Train Acc. 0.9467 | Validation Acc. 0.9474 \n","Epoch 00080 | Loss 0.2612 | Train Acc. 0.9586 | Validation Acc. 0.8947 \n","Epoch 00081 | Loss 0.1221 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00082 | Loss 0.1482 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00083 | Loss 0.1301 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00084 | Loss 0.1533 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00085 | Loss 0.1731 | Train Acc. 0.9349 | Validation Acc. 0.9474 \n","Epoch 00086 | Loss 0.1344 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00087 | Loss 0.1809 | Train Acc. 0.9408 | Validation Acc. 0.8947 \n","Epoch 00088 | Loss 0.1748 | Train Acc. 0.9527 | Validation Acc. 0.9474 \n","Epoch 00089 | Loss 0.1208 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00090 | Loss 0.1881 | Train Acc. 0.9527 | Validation Acc. 0.9474 \n","Epoch 00091 | Loss 0.1770 | Train Acc. 0.9467 | Validation Acc. 0.9474 \n","Epoch 00092 | Loss 0.1552 | Train Acc. 0.9467 | Validation Acc. 0.9474 \n","Epoch 00093 | Loss 0.2029 | Train Acc. 0.9527 | Validation Acc. 0.9474 \n","Epoch 00094 | Loss 0.0893 | Train Acc. 0.9527 | Validation Acc. 0.9474 \n","Epoch 00095 | Loss 0.1632 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00096 | Loss 0.1541 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00097 | Loss 0.1489 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00098 | Loss 0.2103 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00099 | Loss 0.2260 | Train Acc. 0.9231 | Validation Acc. 0.9474 \n","Epoch 00100 | Loss 0.1377 | Train Acc. 0.9231 | Validation Acc. 0.9474 \n","Epoch 00101 | Loss 0.1541 | Train Acc. 0.9231 | Validation Acc. 0.8421 \n","Epoch 00102 | Loss 0.1571 | Train Acc. 0.9586 | Validation Acc. 0.8421 \n","Epoch 00103 | Loss 0.1383 | Train Acc. 0.9586 | Validation Acc. 0.8947 \n","Epoch 00104 | Loss 0.1239 | Train Acc. 0.9645 | Validation Acc. 0.8421 \n","Epoch 00105 | Loss 0.1373 | Train Acc. 0.9527 | Validation Acc. 0.8421 \n","Epoch 00106 | Loss 0.1078 | Train Acc. 0.9704 | Validation Acc. 0.8947 \n","Epoch 00107 | Loss 0.1130 | Train Acc. 0.9704 | Validation Acc. 0.8421 \n","Epoch 00108 | Loss 0.1537 | Train Acc. 0.9645 | Validation Acc. 0.8421 \n","Epoch 00109 | Loss 0.1892 | Train Acc. 0.9527 | Validation Acc. 0.8947 \n","Epoch 00110 | Loss 0.1365 | Train Acc. 0.9527 | Validation Acc. 0.8947 \n","Epoch 00111 | Loss 0.1095 | Train Acc. 0.9586 | Validation Acc. 0.8947 \n","Epoch 00112 | Loss 0.1138 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00113 | Loss 0.1557 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00114 | Loss 0.1541 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00115 | Loss 0.1168 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00116 | Loss 0.0867 | Train Acc. 0.9467 | Validation Acc. 0.9474 \n","Epoch 00117 | Loss 0.1845 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00118 | Loss 0.1154 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00119 | Loss 0.1832 | Train Acc. 0.9408 | Validation Acc. 0.9474 \n","Epoch 00120 | Loss 0.1337 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00121 | Loss 0.1267 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00122 | Loss 0.1925 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00123 | Loss 0.1532 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00124 | Loss 0.1069 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00125 | Loss 0.1046 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00126 | Loss 0.1670 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00127 | Loss 0.1727 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00128 | Loss 0.1073 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00129 | Loss 0.1117 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00130 | Loss 0.1367 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00131 | Loss 0.1565 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00132 | Loss 0.0789 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00133 | Loss 0.1478 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00134 | Loss 0.1292 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00135 | Loss 0.1016 | Train Acc. 0.9527 | Validation Acc. 0.8947 \n","Epoch 00136 | Loss 0.0927 | Train Acc. 0.9527 | Validation Acc. 0.8421 \n","Epoch 00137 | Loss 0.1231 | Train Acc. 0.9527 | Validation Acc. 0.8421 \n","Epoch 00138 | Loss 0.0874 | Train Acc. 0.9467 | Validation Acc. 0.8421 \n","Epoch 00139 | Loss 0.0634 | Train Acc. 0.9467 | Validation Acc. 0.8421 \n","Epoch 00140 | Loss 0.1036 | Train Acc. 0.9527 | Validation Acc. 0.8421 \n","Epoch 00141 | Loss 0.1440 | Train Acc. 0.9586 | Validation Acc. 0.8421 \n","Epoch 00142 | Loss 0.1605 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00143 | Loss 0.1108 | Train Acc. 0.9408 | Validation Acc. 0.8421 \n","Epoch 00144 | Loss 0.1066 | Train Acc. 0.9349 | Validation Acc. 0.8947 \n","Epoch 00145 | Loss 0.2108 | Train Acc. 0.9704 | Validation Acc. 0.8947 \n","Epoch 00146 | Loss 0.1040 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00147 | Loss 0.1013 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00148 | Loss 0.1211 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00149 | Loss 0.2113 | Train Acc. 0.9586 | Validation Acc. 0.9474 \n","Epoch 00150 | Loss 0.1291 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00151 | Loss 0.0950 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00152 | Loss 0.1091 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00153 | Loss 0.1064 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00154 | Loss 0.0946 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00155 | Loss 0.1229 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00156 | Loss 0.1660 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00157 | Loss 0.1057 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00158 | Loss 0.1450 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00159 | Loss 0.1278 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00160 | Loss 0.1008 | Train Acc. 0.9645 | Validation Acc. 0.9474 \n","Epoch 00161 | Loss 0.1314 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00162 | Loss 0.1102 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00163 | Loss 0.0873 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00164 | Loss 0.1250 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00165 | Loss 0.1002 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00166 | Loss 0.0995 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00167 | Loss 0.1307 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00168 | Loss 0.0697 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00169 | Loss 0.0774 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00170 | Loss 0.0833 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00171 | Loss 0.0960 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00172 | Loss 0.0735 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00173 | Loss 0.0840 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00174 | Loss 0.0941 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00175 | Loss 0.1511 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00176 | Loss 0.0808 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00177 | Loss 0.1037 | Train Acc. 0.9704 | Validation Acc. 0.9474 \n","Epoch 00178 | Loss 0.1358 | Train Acc. 0.9527 | Validation Acc. 0.8947 \n","Epoch 00179 | Loss 0.1050 | Train Acc. 0.9586 | Validation Acc. 0.8947 \n","Epoch 00180 | Loss 0.1032 | Train Acc. 0.9586 | Validation Acc. 0.8947 \n","Epoch 00181 | Loss 0.1111 | Train Acc. 0.9586 | Validation Acc. 0.8947 \n","Epoch 00182 | Loss 0.0838 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00183 | Loss 0.0844 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00184 | Loss 0.0938 | Train Acc. 0.9645 | Validation Acc. 0.8947 \n","Epoch 00185 | Loss 0.0827 | Train Acc. 0.9704 | Validation Acc. 0.8947 \n","Epoch 00186 | Loss 0.1146 | Train Acc. 0.9763 | Validation Acc. 0.8947 \n","Epoch 00187 | Loss 0.1117 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00188 | Loss 0.0985 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00189 | Loss 0.1324 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00190 | Loss 0.0502 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00191 | Loss 0.0726 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00192 | Loss 0.0946 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00193 | Loss 0.0733 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00194 | Loss 0.1312 | Train Acc. 0.9763 | Validation Acc. 0.9474 \n","Epoch 00195 | Loss 0.1277 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00196 | Loss 0.1006 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00197 | Loss 0.0866 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00198 | Loss 0.0964 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00199 | Loss 0.0933 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00200 | Loss 0.0708 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00201 | Loss 0.2140 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00202 | Loss 0.0695 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00203 | Loss 0.0650 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00204 | Loss 0.0861 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00205 | Loss 0.1470 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00206 | Loss 0.1045 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00207 | Loss 0.0760 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00208 | Loss 0.0824 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00209 | Loss 0.1210 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00210 | Loss 0.0758 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00211 | Loss 0.0834 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00212 | Loss 0.0699 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00213 | Loss 0.0799 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00214 | Loss 0.0703 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00215 | Loss 0.0376 | Train Acc. 1.0000 | Validation Acc. 0.8947 \n","Epoch 00216 | Loss 0.1179 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00217 | Loss 0.0700 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00218 | Loss 0.0993 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00219 | Loss 0.0605 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00220 | Loss 0.0489 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00221 | Loss 0.0738 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00222 | Loss 0.0868 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00223 | Loss 0.1281 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00224 | Loss 0.1018 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00225 | Loss 0.0871 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00226 | Loss 0.0926 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00227 | Loss 0.0850 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00228 | Loss 0.0663 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00229 | Loss 0.0490 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00230 | Loss 0.0660 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00231 | Loss 0.0561 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00232 | Loss 0.0722 | Train Acc. 0.9822 | Validation Acc. 0.8947 \n","Epoch 00233 | Loss 0.1037 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00234 | Loss 0.0923 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00235 | Loss 0.0833 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00236 | Loss 0.0410 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00237 | Loss 0.0814 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00238 | Loss 0.0750 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00239 | Loss 0.0784 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00240 | Loss 0.1356 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00241 | Loss 0.0772 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00242 | Loss 0.0840 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00243 | Loss 0.1075 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00244 | Loss 0.0913 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00245 | Loss 0.0547 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00246 | Loss 0.0883 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00247 | Loss 0.1935 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00248 | Loss 0.0995 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00249 | Loss 0.1256 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00250 | Loss 0.0673 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00251 | Loss 0.0645 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00252 | Loss 0.1498 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00253 | Loss 0.1033 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00254 | Loss 0.0969 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00255 | Loss 0.0459 | Train Acc. 0.9822 | Validation Acc. 0.9474 \n","Epoch 00256 | Loss 0.0862 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00257 | Loss 0.0907 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00258 | Loss 0.1000 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00259 | Loss 0.0936 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00260 | Loss 0.0837 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00261 | Loss 0.0594 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00262 | Loss 0.0654 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00263 | Loss 0.0995 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00264 | Loss 0.0588 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00265 | Loss 0.0671 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00266 | Loss 0.0680 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00267 | Loss 0.1084 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00268 | Loss 0.0953 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00269 | Loss 0.0743 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00270 | Loss 0.0868 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00271 | Loss 0.0992 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00272 | Loss 0.0877 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00273 | Loss 0.0686 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00274 | Loss 0.0799 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00275 | Loss 0.0836 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00276 | Loss 0.0996 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00277 | Loss 0.0824 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00278 | Loss 0.0877 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00279 | Loss 0.0726 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00280 | Loss 0.1263 | Train Acc. 1.0000 | Validation Acc. 0.9474 \n","Epoch 00281 | Loss 0.1666 | Train Acc. 1.0000 | Validation Acc. 0.9474 \n","Epoch 00282 | Loss 0.1176 | Train Acc. 1.0000 | Validation Acc. 0.8947 \n","Epoch 00283 | Loss 0.0546 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00284 | Loss 0.0703 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00285 | Loss 0.0668 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00286 | Loss 0.0950 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00287 | Loss 0.0881 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00288 | Loss 0.1075 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00289 | Loss 0.0953 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00290 | Loss 0.1070 | Train Acc. 0.9941 | Validation Acc. 0.8421 \n","Epoch 00291 | Loss 0.0958 | Train Acc. 0.9941 | Validation Acc. 0.8421 \n","Epoch 00292 | Loss 0.0928 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00293 | Loss 0.1060 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00294 | Loss 0.0785 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00295 | Loss 0.0829 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00296 | Loss 0.0556 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00297 | Loss 0.1612 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00298 | Loss 0.0725 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00299 | Loss 0.1138 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00300 | Loss 0.1013 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00301 | Loss 0.1956 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00302 | Loss 0.0521 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00303 | Loss 0.0801 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00304 | Loss 0.1004 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00305 | Loss 0.0767 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00306 | Loss 0.0819 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00307 | Loss 0.0829 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00308 | Loss 0.0582 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00309 | Loss 0.0606 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00310 | Loss 0.0459 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00311 | Loss 0.1072 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00312 | Loss 0.0631 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00313 | Loss 0.0684 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00314 | Loss 0.0824 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00315 | Loss 0.0650 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00316 | Loss 0.0560 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00317 | Loss 0.0648 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00318 | Loss 0.1060 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00319 | Loss 0.0640 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00320 | Loss 0.0811 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00321 | Loss 0.0712 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00322 | Loss 0.0815 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00323 | Loss 0.0819 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00324 | Loss 0.0936 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00325 | Loss 0.1098 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00326 | Loss 0.0402 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00327 | Loss 0.1332 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00328 | Loss 0.0577 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00329 | Loss 0.0562 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00330 | Loss 0.0967 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00331 | Loss 0.0569 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00332 | Loss 0.1198 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00333 | Loss 0.1119 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00334 | Loss 0.0728 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00335 | Loss 0.0672 | Train Acc. 0.9941 | Validation Acc. 0.8947 \n","Epoch 00336 | Loss 0.0871 | Train Acc. 0.9882 | Validation Acc. 0.8947 \n","Epoch 00337 | Loss 0.0638 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00338 | Loss 0.0745 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00339 | Loss 0.0622 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00340 | Loss 0.0996 | Train Acc. 0.9941 | Validation Acc. 0.9474 \n","Epoch 00341 | Loss 0.0544 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00342 | Loss 0.0732 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00343 | Loss 0.0610 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00344 | Loss 0.0731 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00345 | Loss 0.0684 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00346 | Loss 0.1696 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00347 | Loss 0.1423 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00348 | Loss 0.1076 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n","Epoch 00349 | Loss 0.0610 | Train Acc. 0.9882 | Validation Acc. 0.9474 \n"]}],"source":["import torch.optim as optim\n","\n","loss_fcn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n","\n","# training loop\n","for epoch in range(350):\n","    model.train()\n","    total_loss = 0\n","    for batch, (batched_graph, labels) in enumerate(train_loader):\n","        batched_graph = batched_graph.to(device)\n","        labels = labels.to(device)\n","        feat = batched_graph.ndata.pop(\"attr\")\n","        logits = model(batched_graph, feat)\n","        loss = loss_fcn(logits, labels)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    scheduler.step()\n","    train_acc = evaluate(train_loader, device, model)\n","    valid_acc = evaluate(val_loader, device, model)\n","    print(\n","        \"Epoch {:05d} | Loss {:.4f} | Train Acc. {:.4f} | Validation Acc. {:.4f} \".format(\n","            epoch, total_loss / (batch + 1), train_acc, valid_acc\n","        )\n","    )"],"id":"_aXo23TIb_fi"},{"cell_type":"code","source":["model.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"zJdsbLdNh50M","executionInfo":{"status":"error","timestamp":1666876038666,"user_tz":-480,"elapsed":451,"user":{"displayName":"Siddarth Natarajan","userId":"10046017371397858517"}},"outputId":"ee1f52bc-3449-4da6-fab3-611fdc676685"},"id":"zJdsbLdNh50M","execution_count":15,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-56ba11e2607b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'GIN' object has no attribute 'test'"]}]}],"metadata":{"kernelspec":{"display_name":"COMP4222","language":"python","name":"comp4222"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}